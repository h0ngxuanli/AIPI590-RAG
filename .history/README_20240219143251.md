# Motivation


The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.

An idea is to make this challenge a little easier by converting it to an open book science exam using semantic search and Wikipedia.

# Dataset

## data source
We obtain the plain text version of the latest dump from Wikipedia

Data source: [Wikipedia:Database download](https://en.wikipedia.org/wiki/Wikipedia:Database_download#). Wikipedia offers free copies of all available content to interested users. These databases can be used for mirroring, personal use, informal backups, offline use or database queries.

## 

Use [WikiExtractor](https://github.com/attardi/wikiextractor/wiki) extracts text from [Wikipedia dumps](https://dumps.wikimedia.org/). 


# Pipeline 


Retrievers: Get context data
Corpus: we created a custom STEM corpus by filtering wiki articles based on their category metadata
Generated synthetic MCQs using GPT-3.5, GPT4, LLaMA2 70b & Falcon 180b, which were used to train both retrievers and readers (downstream MCQ models)
Reranked top 10 retrieved chunks using a cross-encoder and provided top 2 to 4 re-ranked chunks to readers to solve the MCQs.