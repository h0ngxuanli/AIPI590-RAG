{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install wikipedia-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get root pages from supercategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "import pandas as pd \n",
    "from wikiextractor.clean import clean_markup\n",
    "\n",
    "def get_categorymembers(categorymembers, level=0, max_level=1, root_pages=None):\n",
    "    \"\"\"\n",
    "    Recursively retrieves titles of pages within a Wikipedia category, including titles from nested subcategories up to a specified depth.\n",
    "\n",
    "    :param categorymembers: A dictionary-like object containing category members from the wikipediaapi.\n",
    "    :param level: The current level of depth in category traversal. Defaults to 0.\n",
    "    :param max_level: The maximum depth for category traversal. Defaults to 1.\n",
    "    :param root_pages: A list to store the titles of pages. Defaults to None, in which case it initializes to an empty list.\n",
    "\n",
    "    :return: A list containing the titles of all pages and subcategory pages up to the specified depth.\n",
    "    \"\"\"\n",
    "    if root_pages is None:\n",
    "        root_pages = []\n",
    "\n",
    "    for c in categorymembers.values():\n",
    "        if c.ns == wikipediaapi.Namespace.CATEGORY and level < max_level:\n",
    "            get_categorymembers(c.categorymembers, level=level + 1, max_level=max_level, root_pages=root_pages)\n",
    "        else:\n",
    "            if c.title.startswith('Category'):\n",
    "                try:\n",
    "                    title  = c.title.split(\":\")[1]\n",
    "                except:\n",
    "                    continue\n",
    "            else:\n",
    "                title = c.title\n",
    "            root_pages.append(title)\n",
    "    return root_pages\n",
    "\n",
    "def get_root_pages_from_category(category_name):\n",
    "    \"\"\"\n",
    "    Retrieves the root pages from a specified Wikipedia category.\n",
    "    \n",
    "    :param category_name: The name of the Wikipedia category from which to retrieve pages.\n",
    "\n",
    "    :return: A list of root page titles from the specified Wikipedia category.\n",
    "    \"\"\"\n",
    "    cat = wiki_wiki.page(f\"Category:{category_name}\")\n",
    "    return get_categorymembers(cat.categorymembers)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# load wiki\n",
    "API_KEY = 'ProjName (email)'\n",
    "wiki_wiki = wikipediaapi.Wikipedia(API_KEY, 'en')\n",
    "\n",
    "# select super domains\n",
    "domains = [\"Statistics\", \"Artificial intelligence\", \"Computational mathematics\", \"Numerical analysis\", \"Applied mathematics\", \"Probability\"]\n",
    "root_pages = []\n",
    "\n",
    "# load all root pages\n",
    "for domain in domains:\n",
    "    root_page = get_root_pages_from_category(domain)\n",
    "    root_pages += root_page\n",
    "\n",
    "root_pages = set(root_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get page content from all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# convert data into dataframe and save it into parquet file\n",
    "wiki_data = {\"topic\":[], \"text\":[]}\n",
    "for page in root_pages:\n",
    "   wiki_data[\"topic\"].append(page)\n",
    "   wiki_data[\"text\"].append(wiki_wiki.page(page).text)\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_data)\n",
    "wiki_df = wiki_df.query(\"text.str.len() != 0\")\n",
    "wiki_df = wiki_df.reset_index(drop=True)\n",
    "wiki_df.to_parquet('wiki_ml.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean page text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_noise_section(strings):\n",
    "    \"\"\"\n",
    "    Identifies the index of a 'noise' section in a list of strings.\n",
    "\n",
    "    :param strings: A list of strings, typically representing lines or sections of text.\n",
    "\n",
    "    :return: The index of the first occurrence of a 'noise' section.\n",
    "    \"\"\"\n",
    "    for index, string in enumerate(strings):\n",
    "        if string.startswith(\"See also.\") or string.startswith(\"External links.\") or string.startswith(\"References.\") or string.startswith(\"Bibliography.\"):\n",
    "            return index\n",
    "    return -1\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and reformats the provided text by processing paragraphs and removing certain noise sections.\n",
    "\n",
    "    :param text: The text to be cleaned, typically containing structured or semi-structured content.\n",
    "\n",
    "    :return: The cleaned and reformatted text without the identified noise sections.\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = list(clean_markup(text, ignore_headers=False))\n",
    "    \n",
    "    # text = [paragraph for paragraph in paragraphs if len(paragraph) > 3]\n",
    "    text = []\n",
    "    for paragraph in paragraphs:\n",
    "        if len(paragraph) > 5 or len(text) == 0:\n",
    "            text.append(paragraph)\n",
    "        else:\n",
    "            text[-1] = text[-1] + \" \" + \" \".join(paragraph.split(\" \"))\n",
    "    text = \"\\n\".join(text)\n",
    "    \n",
    "    index = find_noise_section(text.split(\"## \"))\n",
    "    if index!= -1:\n",
    "        text = text.split(\"## \")[:index]\n",
    "        return \" ##\".join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# save file to parquet file\n",
    "wiki_df = pd.read_parquet('./data/wiki_ml_mediawiki.parquet')\n",
    "wiki_df.text = wiki_df.text.apply(clean_text)\n",
    "wiki_df[\"id\"] = wiki_df.index\n",
    "wiki_df.to_parquet('./data/wiki_ml_mediawiki_cleaned.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
