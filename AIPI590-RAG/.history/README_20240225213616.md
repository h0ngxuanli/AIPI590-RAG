# Motivation


The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.

An idea is to make this challenge a little easier by converting it to an open book science exam using semantic search and Wikipedia.

# Dataset

## data source
We obtain the plain text version of the latest dump from Wikipedia

Data source: [Wikipedia:Database download](https://en.wikipedia.org/wiki/Wikipedia:Database_download#). Wikipedia offers free copies of all available content to interested users. These databases can be used for mirroring, personal use, informal backups, offline use or database queries.

## 

Use [WikiExtractor](https://github.com/attardi/wikiextractor/wiki) extracts text from [Wikipedia dumps](https://dumps.wikimedia.org/). 


# Pipeline 


Retrievers: Get context data
Corpus: we created a custom STEM corpus by filtering wiki articles based on their category metadata
Generated synthetic MCQs using GPT-3.5, GPT4, LLaMA2 70b & Falcon 180b, which were used to train both retrievers and readers (downstream MCQ models)
Reranked top 10 retrieved chunks using a cross-encoder and provided top 2 to 4 re-ranked chunks to readers to solve the MCQs.

## 1. Retrievers: Get Context Data
We adopted the standard retrieve & re-rank pipeline to find MCQ specific relevant text chunks from a custom STEM wikipedia corpus.

### 1.1 STEM Wiki Corpus
To address frequent rendering issues (number, equations & symbols) and to filter out irrelevant articles from the existing wiki corpuses, we created a custom STEM wiki corpus as below:

Define a set of seed wikipedia categories related to STEM topics such as Category:Concepts in physics, Category:Physical quantities, etc.
For each category, recursively collect the member pages and subcategories up to a certain depth.
Extract the page contents of the collected wiki URLs Wikipedia-API (~500k pages).

**Chunking**: We first split the full text from each article based on different sections. The longer sections were further broken down into smaller chunks containing approximately 300 tokens. We maintained two representations for each chunk:

A short representation without overlap - used for embedding and search
A longer representation with overlap from the previous and next chunks - used as contexts in the downstream MCQ models.
1.2 Retriever
Retrieved text chunks from pre-trained embedding models such as thenlper/gte-base, BAAI/bge-base-en-v1.5 provided a strong performance boost for the downstream MCQ models. In our pipeline, we observed further performance improvement by fine-tuning the embedding models. We re-purposed the synthetically generated MCQs for the retrieval task, as illustrated in the figure below:



# OCR Project

## Project Structure
The project is organized into 5 main directories: `Models`, `Results`, `UI`,`Notebooks`, and `Data`.

- `Models`: Contains the models for each OCR method tested.
- `Results`: Stores the output from each of the OCR methods.
- `UI`: Houses the Streamlit `UI.py` file that runs a web server.
- `Notebooks`: Contains notebooks for each OCR method.
- `Data`: Contains our custom compiled dataset of images of ingredient lists and backs of various packaged food products.

## Overview
This project focuses on the extraction of text from images using various Optical Character Recognition (OCR) methods. The goal is to accurately and efficiently convert image-based text into machine-readable text. The OCR method extracts the text, and our web server component uses ChaTGPT to extract the relevant information (allergies and dietary restrictions) from the text.

## OCR Methods
We have explored and implemented several OCR methods in this project:

1. **GOCR**: A traditional OCR approach that provides a baseline for our experiments.
2. **Tesseract**: A popular OCR engine that uses deep learning techniques.
3. **ChatGPT Vision**: A novel approach that directly converts images to text.
4. **EasyOCR Model**: A model from the EasyOCR library that we have fine-tuned on our dataset.
5. **MMOCR Model**: A highly sophisticated library, leveraging state-of-the-art models to optimize images for text detection and recognition tasks.
6. **Final Model**: Our custom model that combines Tesseract and a data transformation pipeline. This model preprocesses the image, applies OCR using Tesseract, and then processes the OCR text with ChatGPT to extract the ingredient list.

Each of these methods has corresponding files in the `Models` and `Notebooks` folders.

Run `make install` to install dependencies. This installs dependencies located in `requirements.txt`. `cd` into `Models` to run each model.

## User Interface
We have developed a user interface using Streamlit. The `UI.py` file in the `UI` folder runs a web server that deploys our basic pipeline. Users can upload images and receive back an allergy list.

## How to Run
1. python -m venv venv
2. source venv/bin/activate
3. pip install -r requirements.txt
4. add OpenAI API_EKY to the `.env` file
5. streamlit run UI/UI.py


## Results
The `Results` folder contains the output from each of the OCR methods tested. This allows us to compare the performance of each method and make informed decisions about which methods to use or further develop.

## Weights & Biases Integration
We have documented our testing, tabulation, reports, and comparisons between our different approaches on the Weights & Biases platform. You can view our project here: https://wandb.ai/aipi549/aipi540/overview?workspace=user-hongxuanli and the [Results from all of the models on our dataset](https://wandb.ai/aipi549/aipi540/reports/OCR-BenchMark--Vmlldzo2ODEwMjU2?accessToken=rr7c538ke1glhoocgq9zc1c12mxwon7i4rrqojrpef8hm7m6nfzh3s2u7f3f6q61)

## Slides
[Access to our slides](https://docs.google.com/presentation/d/15MNsctdNaPQHyF_QeDA81SoFeIv6plIoP98E4iScOr0/edit?usp=sharing)