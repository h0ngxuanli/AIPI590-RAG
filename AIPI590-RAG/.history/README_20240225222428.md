# Motivation

The blow figure shows the performance of LLaMA-2-7B on the subsets of MMLU dataset. We can find that LLaMA-2-7B is weak in STEM area compared with the other two. In the Machine Learning domain, LLaMA-2-7B  only achieves less than 50% accuracy. This project aims to probe whether utilizing RAG could imporve LLaMA-2-7B's model performance in Machine Learning QA task.

![MMLU](./pics/compare_domains_mmlu_llama-2-7b.png)

# Dataset

## Wiki Machine Learning Corpus

We scrape the plain text from Wikipedia with [WikiAPI](https://github.com/lehinevych/MediaWikiAPI). ``Statistics``, ``Artificial intelligence``, ``Computational mathematics``, ``Numerical analysis``, ``Applied mathematics``, ``Probability`` supercategories that include Machine Learning knowledges are selected as text resources. For each category, recursively collect the root pages.
Extract the page contents of the collected wiki URLs Wikipedia-API (~7k pages).

![wiki_super](./pics/wiki_super.png)

## MMLU Machine Learning QA subset

We utilize the test set in [MMLU](https://huggingface.co/datasets/lukaemon/mmlu/viewer/machine_learning/train) as our RAG system test set. The train set in it is utilized to as few-shot samples included in the prompt.

# Pipeline 
![MMLU](./pics/pipeline.png)


## 1. Retrievers: Get Context Data
We adopted the standard retrieve & re-rank pipeline to find MCQ specific relevant text chunks from a custom STEM wikipedia corpus.

### 1.1 STEM Wiki 
To address frequent rendering issues (number, equations & symbols) and to filter out irrelevant articles from the existing wiki corpuses, we created a custom STEM wiki corpus as below:



**Chunking**: We first split the full text from each article based on different sections. The longer sections were further broken down into smaller chunks containing approximately 300 tokens. We maintained two representations for each chunk:

A short representation without overlap - used for embedding and search
A longer representation with overlap from the previous and next chunks - used as contexts in the downstream MCQ models.
1.2 Retriever
Retrieved text chunks from pre-trained embedding models such as thenlper/gte-base, BAAI/bge-base-en-v1.5 provided a strong performance boost for the downstream MCQ models. In our pipeline, we observed further performance improvement by fine-tuning the embedding models. We re-purposed the synthetically generated MCQs for the retrieval task, as illustrated in the figure below:








## Results
As shown in the results, the generated context information successfully support the machine learning question.
![MMLU](./pics/results.png) 
