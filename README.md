# Motivation


The dataset for this challenge was generated by giving gpt3.5 snippets of text on a range of scientific topics pulled from wikipedia, and asking it to write a multiple choice question (with a known answer), then filtering out easy questions.

An idea is to make this challenge a little easier by converting it to an open book science exam using semantic search and Wikipedia.

# Dataset

## data source
We obtain the plain text version of the latest dump from Wikipedia

Data source: [Wikipedia:Database download](https://en.wikipedia.org/wiki/Wikipedia:Database_download#). Wikipedia offers free copies of all available content to interested users. These databases can be used for mirroring, personal use, informal backups, offline use or database queries.

## 

Use [WikiExtractor](https://github.com/attardi/wikiextractor/wiki) extracts text from [Wikipedia dumps](https://dumps.wikimedia.org/). 


# Pipeline 


Retrievers: Get context data
Corpus: we created a custom STEM corpus by filtering wiki articles based on their category metadata
Generated synthetic MCQs using GPT-3.5, GPT4, LLaMA2 70b & Falcon 180b, which were used to train both retrievers and readers (downstream MCQ models)
Reranked top 10 retrieved chunks using a cross-encoder and provided top 2 to 4 re-ranked chunks to readers to solve the MCQs.

## 1. Retrievers: Get Context Data
We adopted the standard retrieve & re-rank pipeline to find MCQ specific relevant text chunks from a custom STEM wikipedia corpus.

### 1.1 STEM Wiki Corpus
To address frequent rendering issues (number, equations & symbols) and to filter out irrelevant articles from the existing wiki corpuses, we created a custom STEM wiki corpus as below:

Define a set of seed wikipedia categories related to STEM topics such as Category:Concepts in physics, Category:Physical quantities, etc.
For each category, recursively collect the member pages and subcategories up to a certain depth.
Extract the page contents of the collected wiki URLs Wikipedia-API (~500k pages).

**Chunking**: We first split the full text from each article based on different sections. The longer sections were further broken down into smaller chunks containing approximately 300 tokens. We maintained two representations for each chunk:

A short representation without overlap - used for embedding and search
A longer representation with overlap from the previous and next chunks - used as contexts in the downstream MCQ models.
1.2 Retriever
Retrieved text chunks from pre-trained embedding models such as thenlper/gte-base, BAAI/bge-base-en-v1.5 provided a strong performance boost for the downstream MCQ models. In our pipeline, we observed further performance improvement by fine-tuning the embedding models. We re-purposed the synthetically generated MCQs for the retrieval task, as illustrated in the figure below: